---
title: "Final Report"
author: "Nick Mandarano and Patrick McHugh"
date: "4/26/2022"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Loading Libraries
library(tidyverse)
library(leaps)
library(MASS)
library(ggplot2)
library(gridExtra)
library(car)
library(dplyr)
library(readr)
library(corrplot)
library(sur)
library(caret)
library(plot.matrix)
```

## Introduction

To recap our proposal, we chose to do an NBA-related project and decided to fit a regression model for salary of NBA players, using statistics and other information about the players as covariates.

Our data includes many seasonal statistics for each player, including both simple stats and advanced metrics. We also have access to other personal information for each player, such as team, position, height, etc.

We are modeling based on the 2015-16 NBA season. We know salary is known before the season starts and statistics are created, so it is not a response variable in the traditional sense of a causal effect. We attempt to explore the relationship between salary and the covariates, and aim to prescribe a true mean function, and identify players who may be overperforming or underperforming their contract according to the 2015-16 market; i.e., putting up better or worse statistics than one might expect a player on their salary to do.

## Pre-Modeling

A description of the data sources and variables can be found in the previously submitted EDA portion of the project. We proceeded to filter out players who were below the league minimum in salary, as they were exclusively players signed to short term (i.e., 10-day) contracts with wildly volatile data.

From our EDA, we concluded that a log transformation of the response variable, \texttt{salary}, would be appropriate based on its skewed distribution.

We also noticed that many of the covariates had skewed distributions, or did not have linear marginal relationships with the response variable \texttt{log(salary)}. We attempted to make these variables approximately normally distirbutied with an approximately linear marginal relationship with the response. We experimented with log, square root, and squaring transformations. In the end, we consider the following transformations in our model:

* VORP: A min/max normalization between 0 and 1 to eliminate negative values; then a log transformation
* OWS: A min/max normalization between 0 and 1 to eliminate negative values; then a log transformation
* DWS: Square root transformation
* PTS: Square root transformation
* FTr: Square root transformation
* BLK: Log transformation
* STL: Log transformation
* GS: Log transformation
* ORB: Log transformation

```{r, echo=F}
data_init <- read.csv("data/PlayerData.csv")[,-1] %>% mutate(logsal = log(SALARY))
data <- data_init %>% dplyr::select(logsal, Age, G, gs_adj, MP, PER, X3PAr, ftr_adj, 
                                    orb_adj, DRB., AST., stl_adj, blk_adj, TOV., USG., 
                                    ORtg, DRtg, ows_adj, dws_adj, WS.48, OBPM, DBPM, 
                                    vorp_adj, Pos, Height, pts_adj, TS., International, 
                                    Conference)
attach(data)
```

We can also see that \texttt{Pos} has some redundancy and maybe too much granularity. For example, "F-C" (forward/center) and "C-F" (center/forward) are treated as different positions by the model when they are functionally the same. We cleaned this up by grouping players into "Guards", "Wings", and "Bigs" according to \texttt{Pos}. From the graph, it is unclear if there is a significant relationship between the refined position predictor \text{Pos_cat} and \text{log(salary)}.

```{r p2b}
pos_box = data_init %>% ggplot(aes(x = Pos, y = logsal)) + geom_boxplot() + ggtitle("log(salary) by position (unrefined)")
pos_cat_box = data_init %>% ggplot(aes(x = Pos_cat, y = logsal)) + geom_boxplot() + ggtitle("log(salary) by position (refined)")
grid.arrange(pos_box, pos_cat_box, nrow = 1)
```

We use \texttt{VORP} as an example below. We see the distribution of \texttt{VORP} and its plot against \texttt{log(salary)} both pre- and post-transformation:

```{r p0, echo=F, warning=F, message=F}
vorp_hist = data_init %>% ggplot(aes(x=VORP)) + geom_histogram() + ggtitle("Histogram of VORP")
vorp_scatter = data_init %>% ggplot(aes(x=VORP)) + geom_point(aes(y=logsal)) + ggtitle("Scatterplot of VORP vs. log salary")
adj_vorp_hist = data_init %>% ggplot(aes(x=vorp_adj)) + geom_histogram() + ggtitle("Histogram of transformed VORP")
adj_vorp_scatter = data_init %>% ggplot(aes(x=vorp_adj)) + geom_point(aes(y=logsal)) + ggtitle("Scatterplot of transformed VORP vs. log salary")
grid.arrange(vorp_hist, vorp_scatter, adj_vorp_hist, adj_vorp_scatter, nrow = 2)
```

Though not perfect, the transformed variable appears to be much more appropriate for a linear model than the raw variable. Once our predictors are appropriately transformed, we consider all pairwise correlations between continuous covariates as an initial search for possibly collinearity. 

## Variable Selection

Three pairs of predictors with noticably large correlations according the the correlation matrix are further investigated. We learn that \texttt{pts_adj} and \texttt{MP} have a correlation coefficient of $0.947$, \texttt{ORtg} and \texttt{TS.} have a correlation coefficient of $0.888$, and finally \texttt{WS.48} and \texttt{PER} have a correlation coefficient of $0.864$. To avoid redundancy in the model, we'll remove one predictor in each of the three pairs from the model. The terms' variance inflation factors will guide the decision regarding which predictor to drop. Using GVIF$^\frac{1}{2df}$ will allow the GVIFs to be comparable across dimensions. Thus, we remove \texttt{pts_adj} (8.77 > 6.25), \texttt{TS.} (4.64 > 4.29), and \texttt{WS.48} (8.42 > 7.42).

```{r}
corrplot(cor(data[sapply(data, is.numeric)]))
full = lm(logsal ~ ., data = data)
#vif(full)[,3]
data <- data_init %>% dplyr::select(logsal, Age, G, gs_adj, MP, PER, X3PAr, ftr_adj, orb_adj, DRB., AST.,
                                    stl_adj, blk_adj, TOV., USG., ORtg, DRtg, ows_adj, dws_adj,
                                    OBPM, DBPM, vorp_adj, Pos_cat, Height, International,
                                    Conference, Multiteam)
```


### Initial Modeling and Evaluation

We will begin by looking at a full model with all remaining predictors and employ stepwise regression methods to find the best model. According to BIC, the stepwise regression model working in both directions returns \texttt{MP}, \texttt{Age}, \texttt{USG.}, \texttt{G}, and \texttt{orb_adj} as predictors. We'll call this Model A. The model using AIC in its variable selection process additionally returns \texttt{AST.}, \texttt{stl_adj}, \texttt{Pos_cat}, \texttt{\texttt{Multiteam}, and \texttt{TOV.}. We'll call this Model B.

<!-- **NOTE:** Due to the small nature of our dataset, we will build a model using all available data first, while attempting to be careful to avoid overfitting, and then proceed to formal evaluation techniques such as cross validation later in our analysis. -->

```{r p2, , message=F, results='hide'}

#all_models = regsubsets(SALARY ~ ., force.in = 1, data = data, nbest = max(choose(n_predictors, 0:n_predictors)), really.big = T, nvmax = 13)
null = lm(logsal ~ 1, data = data)
full = lm(logsal ~ ., data = data)
# summary(full)

bic = log(nrow(data))
stepbic_model <- stepAIC(object = null, scope = list(lower = null, upper = full),
direction = "both", k = bic)
stepaic_model <- stepAIC(object = null, scope = list(lower = null, upper = full),
direction = "both", k = 2)

print(stepaic_model$call)
print(stepbic_model$call)
```

Interactions of predictors may also be of interest in our model. Intuitively, it would make sense if the effect of offensive rebounds on a player's salary was dependent on the player's position. Therefore, we'll also consider models with interaction terms, but with 26 possible main effects, there are at least $26(25)=650$ possible interaction terms. We'll use the \texttt{regsubsets} function in the \texttt{leaps} package to return the best models of each size, setting a conservative limit of the number of variables in the model to 50.

```{r}
regfit_full <- regsubsets(logsal ~ .^2, data = data, nvmax = 50, method = "backward")
reg_summary <- summary(regfit_full)

par(mfrow = c(1,2))
plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_summary$bic) # 13
points(bic_min, reg_summary$bic[bic_min], col = "red", cex = 2, pch = 20)
abline(v = c(9,19))
plot(reg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
abline(v = 13, col="red")
abline(v = c(9,19))
```

Of these best models, we see that the model with the smallest BIC is the best model chosen with 13 variables. Though the Adjusted $R^2$ of a model does not necessarily have to increase as more variables are added, we see that in this case, the Adjusted $R^2$ is non-decreasing in the number of variables at least up to 50. Practically, a model with 50 variables is not ideal. There seems to be a point in the Adjusted $R^2$ plot in which the rate of change begins to flatten out that matches well with the point in the BIC plot where the BIC begins to rise again. This point is the best model chosen with 19 variables. Similarly, the best model chosen with 9 variables corresponds to an appropriate lower bound for the number of variables with both a satisfactory BIC and Adjusted $R^2$.

```{r}
coef(regfit_full, 9)
coef(regfit_full, 13)
coef(regfit_full, 19)
```

The best model chosen with 9 variables, which we'll call Model C, includes the following predictors:
* MP
* X3PAr
* ows_adj
* Age:DRtg
* G:ftr_adj
* blk_adj:TOV.
* ORtg:DBPM
* DRtg:vorp_adj
* dws_adj:ConferenceWest

The best model chosen with 13 variables, Model D, includes the aforementioned 9 in addition to:
* Age
* Age:InternationalYes
* AST.:vorp_adj
* blk_adj:ows_adj

The best model chosen with 19 variables, Model E, includes the previously listed 13 as well as :
* ftr_adj
* DRtg
* Age:ows_adj
* G:PER
* ftr_adj:Pos_catWing
* orb_adj:Pos_catWing

However, we would like to follow a rule of thumb that if a predictor is involved in an interaction term within a model, the main effect should also be included. Therefore, we'll add terms to each of the above models in order to suffice this rule. Thus, for the five models proposed so far, we examine the predictors involved.

```{r}
preds <- matrix(double(39*5), nrow = 5)
colnames(preds) <- c(colnames(data)[2:27],"Age:DRtg","G:ftr_adj","blk_adj:TOV.", "ORtg:DBPM","DRtg:vorp_adj","dws_adj:ConferenceWest", "Age:InternationalYes", "AST.:vorp_adj", "blk_adj:ows_adj", "Age:ows_adj", "G:PER", "ftr_adj:Pos_catWing", "orb_adj:Pos_catWing")
rownames(preds) <- c("Model A", "Model B", "Model C", "Model D", "Model E")
preds[1, c(4,1,14,2,8)] <- 1
preds[2, c(4,1,14,2,8,10,11,22,26,13)] <- 1
preds[3, c(4,6,17,27:32,1,16,2,7,12,13,15,20,16,21,18,25)] <- 1
preds[4, c(4,6,17,27:35,1,16,2,7,12,13,15,20,16,21,18,25,1,24,10,21,12,17)] <- 1
preds[5, c(4,5,6,17,27:39,1,16,2,7,12,13,15,20,16,21,18,25,1,24,10,21,12,17,1,17,7,22,8,22)] <- 1

par(mfrow = c(1,1))
plot(preds, asp=TRUE, las = 2, col = c("red","green"), xlab="Variables", ylab="Models")
```

Immediately we can see that all five proposed models use \texttt{Age}, \texttt{G} and \texttt{MP} as predictors. On the other hand, none of the models use \texttt{gs_adj}, \texttt{DRB.}, \texttt{OBPM} or \texttt{Height}.

```{r}
data <- data_init %>% dplyr::select(logsal, Age, G, MP, PER, X3PAr, ftr_adj, orb_adj, AST.,
                                    stl_adj, blk_adj, TOV., USG., ORtg, DRtg, ows_adj, dws_adj,
                                    DBPM, vorp_adj, Pos_cat, International,
                                    Conference, Multiteam)

modA <- stepbic_model
modB <- stepaic_model
modC <- lm(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
             OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
             dws_adj:Conference,
           data = data)
modD <- lm(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
             OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
             dws_adj:Conference + AST. + International + Age:International + AST.:vorp_adj +
             blk_adj:ows_adj,
           data = data)
modE <- lm(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
             OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
             dws_adj:Conference + AST. + International + Age:International + AST.:vorp_adj +
             blk_adj:ows_adj + orb_adj + Pos_cat + Age:ows_adj + G:PER + PER + ftr_adj:Pos_cat +
             orb_adj:Pos_cat,
           data = data)
```

We'll fit each of the models and explore the residual plots.

```{r p3}
modA_resid_plot <- data.frame(resid=modA$residuals, fitted_logsal=modA$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model A residual plot")
modB_resid_plot <- data.frame(resid=modB$residuals, fitted_logsal=modB$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model B residual plot")
modC_resid_plot <- data.frame(resid=modC$residuals, fitted_logsal=modC$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model C residual plot")
modD_resid_plot <- data.frame(resid=modD$residuals, fitted_logsal=modD$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model D residual plot")
modE_resid_plot <- data.frame(resid=modE$residuals, fitted_logsal=modE$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model E residual plot")

grid.arrange(modA_resid_plot, modB_resid_plot, modC_resid_plot, 
             modD_resid_plot,modE_resid_plot, nrow = 2)
```


The residual plots do not appear too problematic. However, a vague downward trend is apparent in each and the variance is not constant across all fitted values. Thus, these are not null plots. Fitting the models using weighted least squares may be of interest in the future.












### Added Variable Plots



After making the adjustments to a few of the predictor variables described above, we continue the iterative process of exploratory model building.
```{r p4c, results='hide'}
data_trimmed <- data_init %>% dplyr::select(logsal, Age, G, gs_adj, MP, PER, X3PAr, ftr_adj, orb_adj, DRB., AST., stl_adj, blk_adj, TOV., USG., ORtg, DRtg, ows_adj, dws_adj, WS.48, OBPM, DBPM, vorp_adj, Pos_cat, Height, pts_adj, TS., International, Multiteam)
trimmed_null = lm(logsal ~ 1, data = data_trimmed)
trimmed_full = lm(logsal ~ ., data = data_trimmed)
#corrplot(cor(data_trimmed[sapply(data_trimmed, is.numeric)]), method="number")
trimmed_aic <- stepAIC(object = trimmed_null, scope = list(lower = trimmed_null, upper = trimmed_full),
direction = "both", k = 2)
summary(trimmed_full)
print(trimmed_aic$call)
trimmed_resid_plot <- data.frame(resid=trimmed_aic$residuals , fitted_logsal=trimmed_aic$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Stepwise AIC model residual plot")
# trimmed_resid_plot
```

## Diagnostics

The residual cloud is similar to those before we cut unnecessary predictors.

### Leverage Points and Cook's Distances

Now, we'll look at some of the highest leverage cases.

```{r p6}
show_diagnostics <- function(model){
  x = model.matrix(model)
  h = x %*% solve(t(x) %*% x) %*% t(x)
  leverages = diag(h)
  lev_resid_plot <- data.frame(resid=model$residuals , fitted_logsal=model$fitted.values, leverage=leverages) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(size=leverages, color=leverages)) + ggtitle("Residual plot with leverages")
  cooks = cooks.distance(model)
  cook_resid_plot <- data.frame(resid=model$residuals , fitted_logsal=model$fitted.values, cook_distance=cooks) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(size=cook_distance, color=cook_distance)) + ggtitle("Residual plot with cooks distance")
  print(lev_resid_plot)
  print(cook_resid_plot)
}

show_diagnostics(trimmed_aic)

```

### Outliers


There's an outlying case with a large leverage and cooks distance; we'll use a t-test for a mean shift to formally test if this is an outlier:
```{r p7}
test_outlier_meanshift <- function(model, ind){
  p = length(coef(model))
  n = nrow(model.matrix(model))
  ri = rstandard(model)[ind]
  t_stat = ri * sqrt(((n - p - 1) / (n - p - (ri ^ 2))))
  pval = pt(t_stat, n - p - 1)
  return (pval)
}

test_outlier_meanshift(trimmed_aic, which.max(cooks.distance(trimmed_aic)))

```


```{r p8, results='hide'}
data_final <- data %>% mutate(logsal = log(SALARY)) %>% dplyr::select(logsal, Age, G, gs_adj, MP, PER, X3PAr, ftr_adj, orb_adj, DRB., AST., stl_adj, blk_adj, TOV., USG., ORtg, DRtg, ows_adj, dws_adj, WS.48, OBPM, DBPM, vorp_adj, Pos_cat, Height, pts_adj, TS., International, Multiteam)
```

A problem thus far has been the heteroscedasticity in the residual plot, along with the downward linear trend of the residuals. We will attempt to use weighted least squares to help resolve this. We looked at a couple different ways of doing this. First we plotted the standardized residual against each of the predictors, and the inverse of each of the predictors, to see if the variance in residual was a function of any of the individual predictors.

```{r p9}
ols_model = trimmed_aic
std_resid = rstandard(ols_model)

resid_vs_x = data_final %>% mutate(rs=std_resid) %>% gather(-rs, key = "some_var_name", value = "some_value_name") %>% ggplot(aes(x = some_value_name, y = rs)) + geom_point() + facet_wrap(~ some_var_name, scales = "free") + ggtitle("Standardized residuals vs predictors") + xlab("predictors")
resid_vs_x_inv = data_final %>% mutate(rs=std_resid) %>% dplyr::select(-Multiteam, -Pos_cat, -International) %>% gather(-rs, key = "some_var_name", value = "some_value_name") %>% ggplot(aes(x = 1/ some_value_name, y = rs)) + geom_point() + facet_wrap(~ some_var_name, scales = "free") + ggtitle("Standardized residuals vs 1/X") + xlab("1/x")
resid_vs_x
resid_vs_x_inv
```

There was no clear relationship between the residuals and any predictor, however. We ended up using the HC3 method, and computing the weights as a function of the OLS residuals and the leverages.


```{r p10}
ols_model = trimmed_aic
std_resid = rstandard(ols_model)

sigmahat_2 = summary(ols_model)$sigma^2
weights = (resid(ols_model))^2 / ((1 - leverage(ols_model))^2)

ols_resid_with_weights = data.frame(resid=ols_model$residuals , fitted_logsal=ols_model$fitted.values, weight=weights) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(color=weight, size=weight)) + ggtitle("OLS Residual plot with weights")

wls_model = lm(logsal ~ Age + pts_adj + Multiteam + dws_adj + Pos_cat, data=data_final, weights=weights)

wls_resids = data.frame(resid=wls_model$residuals , fitted_logsal=wls_model$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("WLS Residual plot")
wls_resids_with_weights = data.frame(resid=wls_model$residuals , fitted_logsal=wls_model$fitted.values, weight=weights) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(color=weight, size=weight)) + ggtitle("WLS Residual plot with weights")
ols_resid_with_weights
wls_resids_with_weights

#summary(wls_model)
#summary(ols_model)
```

We notice that the WLS model performs marginally better than the OLS model, and the residual plot is closer to a null plot without any trends. We will use this method going forward.




## Model Fitting

As we continued the model building process, we wanted to make sure we were avoiding overfitting. We created an algorithm where we used the regsubsets function to identify the n best predictors for all n from 1 to a large number, and included first level interaction effects. While some of these models included interactions without the corresponding main effects, we just used this as a rough guideline. We then looked at some of the evaluation metrics for these "best n predictors" models, such as $R^2$ and MSE, on k-folds cross-validated data. As the number of predictors increased, the model metrics tended to flatten out. While it was good that the cross-validation based metrics weren't getting worse, the rate of improvement decreased heavily. Also, as more terms are added to the model, training set metrics such as $R^2$ and MSE continued to rise. The increasing gap between training metrics and cross-validation metrics concerned us, and guided us towards the simpler models. Also, with similar c-v based evaluation metrics, we prefer simpler models, due to interpretability. This gave us an idea that we needed on the order of 10-30 models 

```{r p11}
train.control <- trainControl(method = "cv", number=5)
datax = data_final %>% dplyr::select(-c(Pos_cat, International))
rsq = c()
rmse = c()nmax = 60
regfit_full <- regsubsets(logsal ~ .^2, data = datax, nvmax = nmax, method = "backward")

for (i in 1:nmax){
  cols = names(which(summary(regfit_full)$which[i,-1] == TRUE))
  predictors <- paste(cols, collapse = "+")
  form = as.formula(paste0("logsal ~", predictors))
  cv.mod = train(form, data=datax, method="lm", trControl = train.control)
  rsq = c(rsq, cv.mod$results$Rsquared)
  rmse = c(rmse, cv.mod$results$RMSE)
}


plt_data = data.frame(n_predictors=1:nmax, RMSE=rmse, Rsq = rsq)
rmse_plt = plt_data %>% ggplot(aes(x=n_predictors, y=RMSE)) + geom_line() + ggtitle("C-V based sqrt(MSE) for a model with n predictors")
rsq_plt = plt_data %>% ggplot(aes(x=n_predictors, y=Rsq)) + geom_line() + ggtitle("C-V based R-squared for a model with n predictors")
grid.arrange(rmse_plt, rsq_plt, nrow=2)

```

### Test for Interaction

### Test for Higher Order

### Model Validation

### Final Model

## Discussion

## Conclusion