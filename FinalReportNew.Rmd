---
title: "Modeling NBA Player Salary in 2015-16"
author: "Nick Mandarano and Patrick McHugh"
date: "4/26/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)


# Loading Libraries
library(tidyverse)
library(leaps)
library(MASS)
library(ggplot2)
library(gridExtra)
library(car)
library(dplyr)
library(readr)
library(corrplot)
library(sur)
library(caret)
library(plot.matrix)
library(patchwork)
```

## Introduction

To recap our proposal, we chose to do an NBA-related project and decided to fit a regression model for the salary of NBA players, using statistics and other information about the players as covariates.

Our data includes many seasonal statistics for each player, including both simple stats and advanced metrics. We also have access to other personal information for each player, such as team, position, height, etc.

We are modeling based on the 2015-16 NBA season. We know that salary is known before the season starts and statistics are created, so it is not a response variable in the traditional sense of a causal effect. We attempt to explore the relationship between salary and the covariates, and aim to prescribe a true mean function, and identify players who may be overperforming or underperforming their contract according to the 2015-16 market; i.e., putting up better or worse statistics than one might expect a player on their salary to do.

## Data Exploration

Our data comes from four different datasets. We used three of Riguang Wen's [datasets](https://figshare.com/articles/dataset/NBA_data/5414170) from figshare.com -- \texttt{players cv}, \texttt{players salary}, and \texttt{players stat}. We also used a [dataset](https://zenodo.org/record/3750329#.YkT6YW7MJAe) called \texttt{NBA RS 2020-1950 Stats} uploaded to zenodo.org by Pablo Gomez and Sandra Giral.

In addition to player salary, the data available to us included statistics from each player for the season, including basic stats such as games played, points, and steals, as well as advanced stats such as Value Over Replacement Player (VORP), Defensive Box +/- (DBPM), and others. Other information included personal and demographic data related to the players, such as age, height, college attended, birthplace, and other things. A full list of variables can be found in the previously submitted EDA description.

We proceeded to filter out players who were below the league minimum in salary, as they were exclusively players signed to short term (i.e., 10-day) contracts with wildly volatile data. We felt that players on shorter than full season contracts would be worth creating a separate model for in another project.

From our EDA, we concluded that a log transformation of the response variable, salary, would be appropriate based on its skewed distribution.

We also noticed that many of the covariates had skewed distributions, or did not have linear marginal relationships with the response variable log(salary). We attempted to make these variables approximately normally distirbutied with an approximately linear marginal relationship with the response. We experimented with log, square root, and squaring transformations. See figure A1 for a list of all transformations made, as well as plots of an example transformation (VORP).

```{r, echo= FALSE , warning=FALSE, message=FALSE}
data_init <- read.csv("data/PlayerData.csv")[,-1] %>% mutate(logsal = log(SALARY))
data <- data_init %>% dplyr::select(logsal, Age, G, gs_adj, MP, PER, X3PAr, ftr_adj, 
                                    orb_adj, DRB., AST., stl_adj, blk_adj, TOV., USG., 
                                    ORtg, DRtg, ows_adj, dws_adj, WS.48, OBPM, DBPM, 
                                    vorp_adj, Pos, Height, pts_adj, TS., International, 
                                    Conference)
attach(data)
```

Though not perfect, the transformed variable appears to be much more appropriate for a linear model than the raw variable. 

We can also see that Pos has some redundancy and maybe too much granularity. For example, "F-C" (forward/center) and "C-F" (center/forward) are treated as different positions by the model when they are functionally the same. We cleaned this up by grouping players into "Guards", "Wings", and "Bigs" according to Pos. Figure A2 shows this variable pre- and post-transformation. From the graph, it is unclear if there is a significant relationship between the refined position predictor Pos_cat and log(salary).

We modified some other categorical variables as well. Over several iterations of model building, we were able to reduce the Team variable to a simple flag, Multiteam. A player appearing with multiple teams over the course of the season had a strong relationship with salary.

Once our predictors are appropriately transformed, we considered all pairwise correlations between continuous covariates as an initial search for possibly collinearity, which we will discuss in more detail later.

## Variable Selection

Figure A5 shows a correlation matrix, previously seen in our EDA. Three pairs of predictors with noticably large correlations according the the correlation matrix are further investigated. We learn that pts_adj and MP have a correlation coefficient of $0.947$, ORtg and TS. have a correlation coefficient of $0.888$, and finally WS.48 and PER have a correlation coefficient of $0.864$. To avoid redundancy in the model, we'll remove one predictor in each of the three pairs from the model. The terms' variance inflation factors will guide the decision regarding which predictor to drop. Using GVIF$^\frac{1}{2df}$ will allow the GVIFs to be comparable across dimensions. Thus, we remove pts_adj (8.77 > 6.25), TS. (4.64 > 4.29), and WS.48 (8.42 > 7.42).

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=6}
full = lm(logsal ~ ., data = data)
#vif(full)[,3]
data <- data_init %>% dplyr::select(logsal, Age, G, gs_adj, MP, PER, X3PAr, ftr_adj, orb_adj, DRB., AST.,
                                    stl_adj, blk_adj, TOV., USG., ORtg, DRtg, ows_adj, dws_adj,
                                    OBPM, DBPM, vorp_adj, Pos_cat, Height, International,
                                    Conference, Multiteam)
```

We will begin by looking at a full model with all remaining predictors and employ stepwise regression methods to find the best model. According to BIC, the stepwise regression model working in both directions returns MP, Age, USG., G and orb_adj as predictors. We'll call this Model A. The model using AIC in its variable selection process additionally returns AST., stl_adj, Pos_cat, Multiteam, and TOV.. We'll call this Model B.

```{r p2, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}

#all_models = regsubsets(SALARY ~ ., force.in = 1, data = data, nbest = max(choose(n_predictors, 0:n_predictors)), really.big = T, nvmax = 13)
null = lm(logsal ~ 1, data = data)
full = lm(logsal ~ ., data = data)
# summary(full)

bic = log(nrow(data))
stepbic_model <- stepAIC(object = null, scope = list(lower = null, upper = full),
direction = "both", k = bic)
stepaic_model <- stepAIC(object = null, scope = list(lower = null, upper = full),
direction = "both", k = 2)

print(stepaic_model$call)
print(stepbic_model$call)
```

Interactions of predictors may also be of interest in our model. Intuitively, it would make sense if the effect of offensive rebounds on a player's salary was dependent on the player's position. Therefore, we'll also consider models with interaction terms, but with 26 possible main effects, there are at least $26(25)=650$ possible interaction terms. We'll use the regsubsets function in the leaps package to return the best models of each size, setting a conservative limit of the number of variables in the model to 50.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
regfit_full <- regsubsets(logsal ~ .^2, data = data, nvmax = 50, method = "backward")
reg_summary <- summary(regfit_full)

par(mfrow = c(1,2))
plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_summary$bic) # 13
points(bic_min, reg_summary$bic[bic_min], col = "red", cex = 2, pch = 20)
abline(v = c(9,19))
plot(reg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
abline(v = 13, col="red")
abline(v = c(9,19))
```

Of these best models previously mentioned, we see that the model with the smallest BIC is the best model chosen with 13 variables. Though the Adjusted $R^2$ of a model does not necessarily have to increase as more variables are added, we see that in this case, the Adjusted $R^2$ is non-decreasing in the number of variables at least up to 50. Practically, a model with 50 variables is not ideal. There seems to be a point in the Adjusted $R^2$ plot in which the rate of change begins to flatten out that matches well with the point in the BIC plot where the BIC begins to rise again. This point is the best model chosen with 19 variables. Similarly, the best model chosen with 9 variables corresponds to an appropriate lower bound for the number of variables with both a satisfactory BIC and Adjusted $R^2$.

As we continued the model building process, we wanted to make sure we were avoiding overfitting. We created an algorithm where we used the regsubsets function to identify the n best predictors for all n from 1 to a large number, and included first level interaction effects. While some of these models included interactions without the corresponding main effects, we just used this as a rough guideline. We then looked at some of the evaluation metrics for these "best n predictors" models, such as $R^2$ and MSE, on k-folds cross-validated data. 

```{r px11, echo=FALSE, fig.width=6, fig.asp=.33}
train.control <- trainControl(method = "cv", number=5)
datax = data %>% dplyr::select(-c(Pos_cat, International, Conference))
rsq = c()
rmse = c()
nmax = 60
regfit_full <- regsubsets(logsal ~ .^2, data = datax, nvmax = nmax, method = "backward")

for (i in 1:nmax){
  cols = names(which(summary(regfit_full)$which[i,-1] == TRUE))
  predictors <- paste(cols, collapse = "+")
  form = as.formula(paste0("logsal ~", predictors))
  cv.mod = train(form, data=datax, method="lm", trControl = train.control)
  rsq = c(rsq, cv.mod$results$Rsquared)
  rmse = c(rmse, cv.mod$results$RMSE)
}


plt_data = data.frame(n_predictors=1:nmax, RMSE=rmse, Rsq = rsq)
rmse_plt = plt_data %>% ggplot(aes(x=n_predictors, y=RMSE)) + geom_line() + ggtitle("C-V based sqrt(MSE) for a model with n predictors")
rsq_plt = plt_data %>% ggplot(aes(x=n_predictors, y=Rsq)) + geom_line() + ggtitle("C-V based R-squared for a model with n predictors")
print(rmse_plt)
cat("\n")
print(rsq_plt)
#grid.arrange(rmse_plt, rsq_plt, nrow=2)

```

As the number of predictors increased, the model metrics tended to flatten out. While it was good that the cross-validation based metrics weren't getting worse, the rate of improvement decreased heavily. Also, as more terms are added to the model, training set metrics such as $R^2$ and MSE continued to rise. The increasing gap between training metrics and cross-validation metrics concerned us, and guided us towards the simpler models. Also, with similar c-v based evaluation metrics, we prefer simpler models, due to interpretability. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
coef(regfit_full, 9)
coef(regfit_full, 13)
coef(regfit_full, 19)
```

The best model chosen with 9 variables, which we'll call Model C, includes the following predictors:

* MP
* X3PAr
* ows_adj
* Age:DRtg
* G:ftr_adj
* blk_adj:TOV.
* ORtg:DBPM
* DRtg:vorp_adj
* dws_adj:ConferenceWest

The best model chosen with 13 variables, Model D, includes the aforementioned 9 in addition to:

* Age
* Age:InternationalYes
* AST.:vorp_adj
* blk_adj:ows_adj

The best model chosen with 19 variables, Model E, includes the previously listed 13 as well as:

* ftr_adj
* DRtg
* Age:ows_adj
* G:PER
* ftr_adj:Pos_catWing
* orb_adj:Pos_catWing

However, we would like to follow a rule of thumb that if a predictor is involved in an interaction term within a model, the main effect should also be included. Therefore, we'll add terms to each of the above models in order to suffice this rule. Thus, for the five models proposed so far, we examine the predictors involved. Figure A4 shows which predictors are used by which models.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
preds <- matrix(double(39*5), nrow = 5)
colnames(preds) <- c(colnames(data)[2:27],"Age:DRtg","G:ftr_adj","blk_adj:TOV.", "ORtg:DBPM","DRtg:vorp_adj","dws_adj:ConferenceWest", "Age:InternationalYes", "AST.:vorp_adj", "blk_adj:ows_adj", "Age:ows_adj", "G:PER", "ftr_adj:Pos_catWing", "orb_adj:Pos_catWing")
rownames(preds) <- c("Model A", "Model B", "Model C", "Model D", "Model E")
preds[1, c(4,1,14,2,8)] <- 1
preds[2, c(4,1,14,2,8,10,11,22,26,13)] <- 1
preds[3, c(4,6,17,27:32,1,16,2,7,12,13,15,20,16,21,18,25)] <- 1
preds[4, c(4,6,17,27:35,1,16,2,7,12,13,15,20,16,21,18,25,1,24,10,21,12,17)] <- 1
preds[5, c(4,5,6,17,27:39,1,16,2,7,12,13,15,20,16,21,18,25,1,24,10,21,12,17,1,17,7,22,8,22)] <- 1
```

All five proposed models use Age, G and MP as predictors. On the other hand, none of the models use gs_adj, DRB., OBPM or Height.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data <- data_init %>% dplyr::select(logsal, Age, G, MP, PER, X3PAr, ftr_adj, orb_adj, AST.,
                                    stl_adj, blk_adj, TOV., USG., ORtg, DRtg, ows_adj, dws_adj,
                                    DBPM, vorp_adj, Pos_cat, International, OBPM,
                                    Conference, Multiteam)

modA <- stepbic_model
modB <- stepaic_model
modC <- lm(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
             OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
             dws_adj:Conference,
           data = data)
modD <- lm(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
             OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
             dws_adj:Conference + AST. + International + Age:International + AST.:vorp_adj +
             blk_adj:ows_adj,
           data = data)
modE <- lm(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
             OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
             dws_adj:Conference + AST. + International + Age:International + AST.:vorp_adj +
             blk_adj:ows_adj + orb_adj + Pos_cat + Age:ows_adj + G:PER + PER + ftr_adj:Pos_cat +
             orb_adj:Pos_cat,
           data = data)
```

## Residual Analysis

```{r p3, echo=FALSE, warning=FALSE, message=FALSE}
modA_resid_plot <- data.frame(resid=modA$residuals, fitted_logsal=modA$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model A residual plot")
modB_resid_plot <- data.frame(resid=modB$residuals, fitted_logsal=modB$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model B residual plot")
modC_resid_plot <- data.frame(resid=modC$residuals, fitted_logsal=modC$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model C residual plot")
modD_resid_plot <- data.frame(resid=modD$residuals, fitted_logsal=modD$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model D residual plot")
modE_resid_plot <- data.frame(resid=modE$residuals, fitted_logsal=modE$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("Model E residual plot")

grid.arrange(modA_resid_plot, modB_resid_plot, modC_resid_plot, 
             modD_resid_plot,modE_resid_plot, nrow = 2)
```

The residual plots do not appear too problematic. However, a vague downward trend is apparent in each and the variance is not constant across all fitted values. Thus, these are not null plots. Fitting the models using weighted least squares may be of interest in the future. One persisting concern is the idea of overfitting the model. In order to evaluate whether any of these models may be victim to overfitting or underfitting, we'll perform 6-fold cross-validation on each model. The number of folds is chosen to be 6 because the data has 366 observations, so the folds will split evenly.

## Cross-Validation

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
set.seed(6950)
ctrl <- trainControl(method = "cv", number = 6)

cv.a <- train(logsal ~ MP + Age + USG. + G + orb_adj,
              data = data,
              method = "lm",
              trControl = ctrl)
cv.b <- train(logsal ~ MP + Age + USG. + G + orb_adj + AST. + stl_adj + Pos_cat + Multiteam + TOV.,
              data = data,
              method = "lm",
              trControl = ctrl)
cv.c <- train(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
                OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
                dws_adj:Conference,
           data = data,
           method = "lm",
           trControl = ctrl)
cv.d <- train(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
                OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
                dws_adj:Conference + AST. + International + Age:International + AST.:vorp_adj +
                blk_adj:ows_adj,
             data = data,
             method = "lm",
             trControl = ctrl)
cv.e <- train(logsal ~ Age + G + MP + X3PAr + ftr_adj + blk_adj + TOV. + ORtg + DRtg + ows_adj + dws_adj +
                OBPM + DBPM + Conference + Multiteam + Age:DRtg + G:ftr_adj + ORtg:DBPM + DRtg:vorp_adj +
                dws_adj:Conference + AST. + International + Age:International + AST.:vorp_adj +
                blk_adj:ows_adj + orb_adj + Pos_cat + Age:ows_adj + G:PER + PER + ftr_adj:Pos_cat +
                orb_adj:Pos_cat,
             data = data,
             method = "lm",
             trControl = ctrl)

cv_res <- matrix(c(print(cv.a)[1,],print(cv.b)[1,],print(cv.c)[1,],print(cv.d)[1,],print(cv.e)[1,]),
                 nrow = 5, byrow = TRUE)
colnames(cv_res) <- c("CV.RMSE", "CV.Rsquared", "CV.MAE")
rownames(cv_res) <- c("Model A", "Model B", "Model C", "Model D", "Model E")

mod_res <- matrix(c(summary(modA)$r.squared, summary(modB)$r.squared, summary(modC)$r.squared,
                    summary(modD)$r.squared, summary(modE)$r.squared, summary(modA)$adj.r.squared,
                    summary(modB)$adj.r.squared, summary(modC)$adj.r.squared, summary(modD)$adj.r.squared,
                    summary(modE)$adj.r.squared),
                  nrow = 5, byrow = FALSE
                  )
colnames(mod_res) <- c("Model.Rsquared", "Model.AdjRsquared")
rownames(mod_res) <- c("Model A", "Model B", "Model C", "Model D", "Model E")

mod_feat <- data.frame(cbind(mod_res, cv_res))
```

```{r, echo = FALSE}
mod_feat
```

Here, Model.Rsquared is the $R^2$ reported by the model and Model.AdjRsquared is the Adjusted $R^2$ reported by the model. Meanwhile, CV.RMSE is the average root mean squared error of the model on unseen data from the cross-validation, CV.Rsquared is the $R^2$ of the model on unseen data from the cross-validation, and CV.MAE is the mean absolute error of the model on unseen data from the cross-validation. Ideally, we're searching for higher values for Model.Rsquared, Model.AdjRsquared and CV.Rsquared with lower values for CV.RMSE and CV.MAE. However, too small of a difference between model accuracy and cross-validation accuracy may be an indication of overfitting, while too large of a difference may be a symptom of underfitting.

Model B performed the best in the cross-validation and we decide to continue our analysis by considering only Model B

## Considering Weighted Least Squares

A previously mentioned concern was the vague downward linear trend of the residuals. We will attempt to use weighted least squares to help resolve this. Our first attempt will involve plotting the standardized residuals against each of the predictors as well as the inverse of each of the predictors. This will help display if the variance in residuals is a function of any of the individual predictors.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# ols_model <- modB
# std_resid = rstandard(ols_model)
# 
# resid_vs_x = data %>% mutate(rs=std_resid) %>% gather(-rs, key = "some_var_name", value = "some_value_name") %>% ggplot(aes(x = some_value_name, y = rs)) + geom_point() + facet_wrap(~ some_var_name, scales = "free") + ggtitle("Standardized residuals vs predictors") + xlab("predictors")
# resid_vs_x_inv = data %>% mutate(rs=std_resid) %>% dplyr::select(-Multiteam, -Pos_cat, -International) %>% gather(-rs, key = "some_var_name", value = "some_value_name") %>% ggplot(aes(x = 1/some_value_name, y = rs)) + geom_point() + facet_wrap(~ some_var_name, scales = "free") + ggtitle("Standardized residuals vs 1/X") + xlab("1/x")
# resid_vs_x
# resid_vs_x_inv
```

No clear relationship between the residuals and any predictor is observed, so we instead use the HC3 method and compute the weights as a function of the OLS residuals and the leverages.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4.8, fig.asp=0.618}
ols_model = modB
std_resid = rstandard(ols_model)

sigmahat_2 = summary(ols_model)$sigma^2
weights = (resid(ols_model))^2 / ((1 - leverage(ols_model))^2)

ols_resid_with_weights = data.frame(resid=ols_model$residuals , fitted_logsal=ols_model$fitted.values, weight=weights) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(color=weight, size=weight)) + ggtitle("OLS Residual plot with weights")

wls_model = lm(as.formula(ols_model$call[2]), weight=weights, data = data)

wls_resids = data.frame(resid=wls_model$residuals , fitted_logsal=wls_model$fitted.values) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point() + ggtitle("WLS Residual plot")
wls_resids_with_weights = data.frame(resid=wls_model$residuals , fitted_logsal=wls_model$fitted.values, weight=weights) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(color=weight, size=weight)) + ggtitle("WLS Residual plot with weights")

ols_resid_with_weights
```


```{r p5y, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4.8, fig.asp=0.618}
wls_resids_with_weights


ols.cv = train(as.formula(ols_model$call[2]), data=data, method="lm", trControl = train.control)
wls.cv = train(as.formula(ols_model$call[2]), data=data, method="lm", weights=weights, trControl = train.control)
cat("OLS:")
print(ols.cv$results)
cat("WLS:")
print(wls.cv$results)
```


We notice that the residual plot for the WLS is marginally better. However, when evaluating each model over k-folds, the WLS performs poorly. We suspect this is probably a case of overfitting. The OLS model is better and it's not even close. Thus, we decided to table the WLS idea for now.


## Model Diagnostics

In all, our model reports an $R^2$ value of 0.5871 and Adjusted $R^2$ value of 0.5742. The 6-fold cross-validation procedure returns an average $R^2$ of 0.5665 on unseen test data, with the smallest root mean squared error and smallest mean absolute error of any of the five proposed models.

We will also look at the residual plot again, as well as the standardized residual plot:

```{r p5z, fig.width=7, fig.asp=0.5, echo=FALSE}
resid_df = data.frame(raw_residuals=modB$residuals , fitted_logsal=modB$fitted.values, std_residuals=rstandard(modB))

raw_resid_plt <- resid_df %>%  ggplot(aes(x=fitted_logsal, y=raw_residuals)) + geom_point() + ggtitle("Raw residual plot")
std_resid_plt <- resid_df %>%  ggplot(aes(x=fitted_logsal, y=std_residuals)) + geom_point() + ggtitle("Standardized residual plot")
grid.arrange(raw_resid_plt, std_resid_plt, nrow=1)
```

While these plots are not perfect, namely in that there appears to be a slightly downward trend and some possible heteroscedasticity, they are reasonable enough to suggest that none of the assumptions of the linear model were flagrantly violated. The residuals are centered around zero, and somewhat resemble a "null plot." There does not appear to be any significant differences between the raw and standardized residuals that are worth discussing/exploring further.

We also looked at some of the cases with the highest leverage and Cook's distance. See Figure A6 for a plot of these leverages and Cook's distances.

```{r p6, echo = FALSE, warning = FALSE, message=FALSE, fig.width=4.8, fig.asp=0.618}
show_diagnostics <- function(model){
  x = model.matrix(model)
  h = x %*% solve(t(x) %*% x) %*% t(x)
  leverages = diag(h)
  lev_resid_plot <- data.frame(resid=model$residuals , fitted_logsal=model$fitted.values, leverage=leverages) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(size=leverages, color=leverages)) + ggtitle("Residual plot with leverages")
  cooks = cooks.distance(model)
  cook_resid_plot <- data.frame(resid=model$residuals , fitted_logsal=model$fitted.values, cook_distance=cooks) %>%  ggplot(aes(x=fitted_logsal, y=resid)) + geom_point(aes(size=cook_distance, color=cook_distance)) + ggtitle("Residual plot with Cook's Distance")
  print(lev_resid_plot)
  cat("\n\n")
  print(cook_resid_plot)
}

```

There's an outlying case with a large leverage and Cook's Distance. The residual does not appear to be anything extraordinary, however. After digging into this data point, we see that it's Jarnell Stokes, a guy who appeared in 7 games and played 18 minutes. It does appear that he has some noisy statistics due to a small sample size from a lack of playing time. We'll use a t-test for a mean shift to formally test if this is an outlier.

```{r p7, echo = FALSE, warning = FALSE, message=FALSE}
test_outlier_meanshift <- function(model, ind){
  p = length(coef(model))
  n = nrow(model.matrix(model))
  ri = rstandard(model)[ind]
  t_stat = ri * sqrt(((n - p - 1) / (n - p - (ri ^ 2))))
  pval = pt(t_stat, n - p - 1)
  return (pval)
}

outlier = data_init[which.max(cooks.distance(modB)),]
pv = test_outlier_meanshift(modB, which.max(cooks.distance(modB)))
cat("Outlier p-value: ", pv)

outlier2 = data_init[which.max(modB$residuals),]

```

This p-value is small, but not ridiculously small in the context of a dataset of 366 observations. We are unable to conclude that this point is an outlier and there is a reason to believe that the mean function for this point should be shifted. 

There is another data point that is a bit of an outlier in terms of having the largest residual. It doesn't actually appear to be an outlier that suggests the mean function assumed by the model is incorrect, however. This player is Iman Shumpert; we'll discuss him a bit later in our conclusions section.

## Discussion

### Model Interpretation

In our model, four predictors are significant at the 99.9% confidence level: MP, Age, G and AST. The estimated coefficients for MP, Age and AST. are positive, which makes sense to a casual basketball fan. Players that play more often and produce higher offensive statistics should be paid more. Additionally, players generally tend to sign larger contracts as their career progresses. However, G has a negative estimated coefficient, implying that on average, players are paid less money when they play in more games. This doesn’t seem to be too logical at first. However, one could argue that a player that plays in more games would also generally play more minutes, and this argument would be backed by the 0.843 correlation coefficient between these two variables in our dataset. Perhaps one of these factors could have been removed early on, but given they are both highly statistically significant in our final model, it’s probably good that they were both kept. The estimated coefficients specifically for MP and for G are 0.009451 and -0.0158993, respectively, implying that a player’s predicted salary increases by approximately 0.09% for every extra minute played and decreases by about 1.58% for every extra game played. Therefore, the salary of a player who averages 18 or more minutes a game is predicted to benefit as the player plays more, while that of a player who averaged 17 minutes or less per game is expected to decrease as the player plays more.

At the 99% confidence level, two more variables are significant: USG. And Pos_catG. USG. has a positive estimated coefficient of 0.0244984, indicating that a player’s expected salary increases 2.48% for every unit increase in USG. The Pos_catG indicator variable, conversely, has an estimated coefficient of -0.3685353. Since this model uses Pos_catBig as the baseline, this model projects that a guard’s predicted salary will be more than 30% less than a center’s given that the guard and center have identical factors for this model otherwise.

Two additional models are significant at the 90% confidence level: orb_adj and stl_adj, with a positive and negative estimated coefficient, respectively. The idea that a player who can accumulate more offensive rebounds is predicted to have a higher salary is completely logical. On the other hand, a player who garners more steals being predicted to have a lower salary may be a bit confusing for basketball fans. A possible explanation for this could be that players with more steals are more likely to be “defense-first” players, which in 2015-16, were not as highly valued as primarily offensive skilled players.

The estimated coefficient for the Pos_catWing indicator variable also becomes significant at the 90% confidence level, with an estimated value of -0.1741437. Like the situation for guards, our model proposes that the predicted salary for a Wing relative to a Center with the same model inputs otherwise is 15.98% lower, on average. The idea that talented bigs were much harder to come by than talented guards or wings during this time in the NBA would be widely agreed upon by fans. This could conceivably be why our model predicts centers with the same model-relevant statistics as a guard or wing would have higher salaries. Similarly, the talent pool was probably deepest among guards at this time, so there was less of an urge to pay guards as much money.

Also present in the model are Multiteam and TOV., though neither significant at the 90% level. The estimated coefficient for the Multiteam indicator variables is positive, suggesting players who played for various teams during the 2015-16 NBA season typically had higher salaries. The estimated coefficient for TOV. was negative, confirming the idea that players who turned the ball over more often were paid less, on average.

```{r, message=FALSE,warning=FALSE}
summary(modB)
```


### Contextual Applications

One thing that was of interest to us was using this model to see which players are overperforming and underperforming their salary, based on the model's expectations. We'll look at the largest and smallest residuals, raw and standardized:

```{r p12, echo=F}
fmod = modB

und_raw = which.min(resid(fmod))
cat("Most Underpaid: ", data_init[und_raw, "Player"], ", Expected Salary: ", exp(predict(fmod, newdata=data[und_raw,])), ", Actual Salary: ", data_init[und_raw, "SALARY"], sep="")

ovp_raw = which.max(resid(fmod))
cat("Most Overpaid: ", data_init[ovp_raw, "Player"], ":, Expected Salary: ", exp(predict(fmod, newdata=data[ovp_raw,])), ", Actual Salary: ", data_init[ovp_raw, "SALARY"], sep="")

```

Our model thought Rodney Hood was the league's most underpaid player, and Iman Shumpert was the league's most overpaid player (using both raw and standardized residuals, for both players). Hood's relatively high usage rate, offensive rebounding rate, and assists led to the model expecting him to have a fairly average salary, when he was actually paid towards the low end of the league. Conversely, Shumpert's classification as a guard, low usage rate and assists, and relatively high turnover rate led the model to think he should be paid fairly low, when in reality he was a well-paid player. We imagine that digging into which players were over/underpaid, and whether or not this had a correlation with team success, would be a fun and useful activity with a trustworthy model.


## Conclusion

In attempting to model the NBA player salary market during the 2015-16 season, we considered many variables and began by eliminating variables we could argue wouldn’t be helpful.  We then proposed five models through different methods of stepwise regression, some with interaction terms and some with only main effects. Through the employment of k-fold cross-validation, we decided on a best model to predict player salaries that includes an intercept and nine main effects.

Probably the strictest limitation to our model is that it is only useful for the 2015-16 NBA season. As the economy, the market, the player pool, and many other factors including what assets NBA teams value in a player change, the salary prediction model is destined to change as well. However, even with this limitation, our model proposes not only a fun conversation starter for basketball fans, but also a jumping off point for future work.

With a prediction model and real data, we can investigate which players the model deemed as overpaid and underpaid in 2015-16. Furthermore, the question of whether overpaying players inhibits your ability to win or whether underpaying players allows more flexibility to win can be addressed from a team perspective.

Secondarily, we can predict what players from other eras would make in 2015-16 if they were dropped into the league with all the same covariates. For instance, if we duplicate one of Michael Jordan’s greatest seasons as a 2015-16 performance, we can predict what Jordan’s salary may have been. Comparing our model’s prediction to his actual salary under the consideration of inflation may support the discussion regarding how quickly and drastically NBA contracts are growing. Performing this type of analysis over a larger group of players could also provide insight to how the values of NBA teams have changed over time by investigating which predictors are more significant in different eras of basketball.

Most importantly, our work provides a notion of how to develop such a model. Despite the limitation mentioned prior, one could use the methods outlined in this paper to construct a similar model for another season, with different potential covariates, or even for an entirely different sport.

Any future advancement of this work should, however, consider the idea of holding out a test set until the very end of the model construction. Such a decision may have provided more validation for our model had we chosen to do so.


\newpage

## Appendix

&nbsp;
&nbsp;

#### Figure A1: Transformed variables; and transformation of VORP  

&nbsp;

* VORP: A min/max normalization between 0 and 1 to eliminate negative values; then a log transformation
* OWS: A min/max normalization between 0 and 1 to eliminate negative values; then a log transformation
* DWS: Square root transformation
* PTS: Square root transformation
* FTr: Square root transformation
* BLK: Log transformation
* STL: Log transformation
* GS: Log transformation
* ORB: Log transformation  
  

```{r p0, echo=FALSE, warning=FALSE, message=FALSE}
vorp_hist = data_init %>% ggplot(aes(x=VORP)) + geom_histogram() + ggtitle("Histogram of VORP")
vorp_scatter = data_init %>% ggplot(aes(x=VORP)) + geom_point(aes(y=logsal)) + ggtitle("Scatterplot of VORP vs. log salary")
adj_vorp_hist = data_init %>% ggplot(aes(x=vorp_adj)) + geom_histogram() + ggtitle("Histogram of transformed VORP")
adj_vorp_scatter = data_init %>% ggplot(aes(x=vorp_adj)) + geom_point(aes(y=logsal)) + ggtitle("Scatterplot of transformed VORP vs. log salary")
grid.arrange(vorp_hist, vorp_scatter, adj_vorp_hist, adj_vorp_scatter, nrow = 2)
```

\newpage

#### Figure A2: Transformation of Pos  
  
&nbsp;

```{r p2b, echo=FALSE, warning=FALSE, message=FALSE}
pos_box = data_init %>% ggplot(aes(x = Pos, y = logsal)) + geom_boxplot() + ggtitle("log(salary) by position (unrefined)")
pos_cat_box = data_init %>% ggplot(aes(x = Pos_cat, y = logsal)) + geom_boxplot() + ggtitle("log(salary) by position (refined)")
grid.arrange(pos_box, pos_cat_box, nrow = 1)
```

#### Output A3: Summaries of Models A, C, D, E

&nbsp;

```{r, warning=FALSE, message=FALSE}
summary(modA)
summary(modC)
summary(modD)
summary(modE)
```

\newpage

#### Figure A4: Predictors used in selected models

&nbsp;
  
```{r warning=F, message=F, echo=F}
par(mfrow = c(1,1))
plot(preds, asp=TRUE, las = 2, col = c("red","green"), xlab="", ylab="", main="Predictors used in selected models")
```

\newpage

#### Figure A5: Correlation Matrix

&nbsp;
  
```{r warning=F, message=F, echo=F}
corrplot(cor(data[sapply(data, is.numeric)]))
```

#### Figure A6: Residual plots of final model, including leverage and Cook's distance

&nbsp;

```{r a6, warning=F, echo=F, message=F}
show_diagnostics(modB)
```
